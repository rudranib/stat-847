---
title: "847A2 Q1"
output: pdf_document
---

## 1.
Suppose we have a continuous random variable $X$ with distribution function $F_X(x) = Pr(X \le x)$ and quantile function
$Q_X(p) = F_X^{-1}(p)$.  That is $p = F_X(x) = Pr(X \le x)$ and $p = Pr(X \le Q_X(p)) = F_X(Q_X(p)) = F_X(F_X^{-1}(p)) = p$.
a.
*(4 marks)* Suppose $Y = aX +b$ for some constants $a > 0$ and $b$. 
        **Prove** that a plot of the parametric curve $(Q_X(p), Q_Y(p))$ for $p \in (0,1)$ must follow a straight line.  
        
        Give the equation of that line.

$Y=aX+b$

$Pr(Y<=y)$ $=$ $Pr(aX+b<=y)$ $=$ $Pr\Big(X<= \frac{y-b}{a}\Big)$

$=>$ $F_{Y}(y)$ $=$ $F_{X}\Big(\frac{y-b}{a}\Big)$

$=>$ $F_{Y}(y)$ $=$ $F_{X}(g(y))$

where $g(y)=\frac{y-b}{a}$

$g^{-1}(x)=ax+b$

$=>$ $F^{-1}(p)$ $=$ $g^{-1}(F^{-1}(p))$

$=>$ $Q_{Y}(p)$ $=$ $aQ_{X}(p) + b$

b.
*(3 marks)* When $F_X(x)$ and $Q_X(p)$ are the cumulative distribution and quantile functions of the continuous random variable $X$, show that if $U \sim U(0,1)$, then 
    
$Pr(Q_X(U) \le x )  = F_X(x).$

$Pr(Q_x(U) <= x)$

$=$ $Pr(Q_x^-1(Q_x(U)) <= F_x(x))$

$=$ $Pr(U <= F_x(x))$

$=$ $\frac{F_x(x)-0}{1-0}$

$=$ $F_x(x)$

c.
The above result implies that we could generate $n$ independently and identically distributed (i.i.d.) random realizations $X$ from $F_X(x)$ by generating $n$ i.i.d. random realizations $U$ from $U(0,1)$ and defining $X = Q_X(U)$.  
    
In R the function `runif()` will generate uniform pseudo-random numbers. 
    
(Similarly, `dunif()`, `punif()`, and `qunif()` will return the density, the distribution, and the quantile functions, respectively, for a uniform random variable.  See `help("runif")` for details.

i.
 *(1 mark)* Write an R function
    
            ```{r r_unigenFx, eval = FALSE}
            r_unifgenFx <- function(n, qfunction = qnorm) {
                                # Insert your code here
                                 }
            ```
        
            which will generate and return `n` pseudo random observations from the distrubution whose quantile function is the value of the argument `qfunction`.  Show your code.

```{r}
r_unifgenFx <- function(n, qfunction = qnorm) {
  qfunction(runif(n))
  }
```

ii.
*(2 marks)*  Execute the following code snippets to illustrate your code
        
            ```{r hists, eval = FALSE}
            # make sure we all get the same result
            set.seed(1234567)
            # save the currentgraphical parameters and set `mfrow`
            oldPar <- par(mfrow = c(1,2))
            
            hist(r_unifgenFx(1000))  # Standard normal
            
            hist(r_unifgenFx(1000, qfunction = runif))'
            par(oldPar) # Return to original graphical parameters
            ```
```{r}
# make sure we all get the same result
set.seed(1234567)
# save the currentgraphical parameters and set `mfrow`
oldPar <- par(mfrow = c(1,2))
hist(r_unifgenFx(1000)) # Standard normal
hist(r_unifgenFx(1000, qfunction = qunif))
par(oldPar) # Return to original graphical parameters
```
iii.
*(2 marks)* Generate a sample of 1000 pseudo-random observations from a 
              Student-t distribution on 3 degrees of freedom  generated using `r_unifgen()` (unchanged) and the quantile function of the Student-t.
              Plot a histogram (appropriately labelled) of the results.

```{r}
hist(qt(r_unifgenFx(1000,qfunction = runif),df=3),main="Histogram of Student-t",
     xlab="T values",col="pink")
```

d.
Consider the `quantile()` function in R.
i.
*(2 marks)* Explain the values returned by `quantile(mtcars$qsec))`.  That is, what does `quantile()` do?
```{r}
quantile(mtcars$qsec)
```
0% shows us the min value of qsec, 100% shows us the maximum qsec value, 25% shows us the qsec value at the 25% of the range of the data, 50% shows us the qsec value at the 50% of the range of the data and 75% shows us the qsec value at the 75% of the range of the data.

ii.
*(2 marks)* Show how `quantile()` could be used to generate 1000 observations from the estimated distribution of `mtcars$qsec`.
```{r}
#mtcars$qsec
h1 <- quantile(mtcars$qsec,prob=runif(1000))
```

iii.
*(2 marks)* Would this work for `mtcars$cyl`? Why? Or, why not?
```{r}
#mtcars$cyl
h2 <- quantile(mtcars$cyl,prob=runif(1000))

hist(h2)
```
No it won't work since the values are discrete and it is categorical data. Defining quantile functions for discrete rather than continuous distributions requires a bit more work since the discrete nature of such a distribution means that there may be gaps between values in the domain of the distribution function.

iv.
*(4 marks)* Draw side by side (nicely labelled) histograms of `mtcars$qsec` and a sample of 1000 observations drawn from the estimated distribution of `mtcars$qsec`.  Comment on how these compare.
```{r}
par(mfrow=c(1,2))
hist(mtcars$qsec,main="Histogram of mtcars$qsec",col="green")
hist(h1,main="Histogram of 1000 samples of mtcars$qsec",col="yellow")
```
Both the histograms follow a normal distribution and are similar to each other since they come from the same distribution.

v.
*(3 marks)* Draw a (nicely labelled) `qqplot()` comparing the above two sets of observations.  What do you conclude about their empirical distributions?  Why?
```{r}
qqplot(mtcars$qsec,h1,xlab="Theoretical Quantiles",ylab="Sample Quantiles")

```
It is almost a straight line with a few outliers which means they come from the same distribution.

vi.
 *(4 marks)* Suppose interest lay in producing a bootstrap distribution for some estimator $\tilde{\theta}$.  Instead of bootstrapping, how might `quantile()` be used?  Which would you recommend and why?
 
A nonparametric bootstrap replication can be generated by taking a random sample of size n from a uniform(0, 1) distribution and applying the sample quantile function to each uniform random variable. By employing this method, they show that various bootstrap-estimated moments of L-estimators that are not easily calculated by considering a continuous distribution F maybe obtained directly, thus eliminating the resampling step in the estimation altogether. This general quantile function approach has been fruitful for solving a variety of difficult analytical bootstrap problems. One place where this method has been useful is for bootstrap problems that deal with censored data. Also, direct calculation can eliminate the need for an outer bootstrap resampling loop in a double bootstrap, making a more efficient algorithm.

